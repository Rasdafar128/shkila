{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92b77025",
      "metadata": {
        "id": "92b77025",
        "papermill": {
          "duration": 0.007863,
          "end_time": "2025-04-06T19:48:45.587327",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.579464",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "**Homework main task**: overcome Mistral hallucinations in chat mode."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "516d8063",
      "metadata": {
        "id": "516d8063",
        "papermill": {
          "duration": 0.00677,
          "end_time": "2025-04-06T19:48:45.601199",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.594429",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Plan:\n",
        "\n",
        "- Test the pre-trained model as an online store assistant.\n",
        "- Completely train the model using the LoRA method on data from chatbot communication with clients on various products.\n",
        "- Test the completed model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb45ca4a",
      "metadata": {
        "id": "eb45ca4a",
        "papermill": {
          "duration": 0.00672,
          "end_time": "2025-04-06T19:48:45.614734",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.608014",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "You need to fill in all the blankes in the code (there are 13 spaces in total)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b62af449",
      "metadata": {
        "id": "b62af449",
        "papermill": {
          "duration": 0.006686,
          "end_time": "2025-04-06T19:48:45.628251",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.621565",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "**Note!** Please safe all cells outputs, otherwise the task will not be accepted!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41fbce47",
      "metadata": {
        "id": "41fbce47",
        "papermill": {
          "duration": 0.006781,
          "end_time": "2025-04-06T19:48:45.641784",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.635003",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Fine-tune Mistral 7B."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e297201",
      "metadata": {
        "id": "2e297201",
        "papermill": {
          "duration": 18.264232,
          "end_time": "2025-04-06T19:49:03.912978",
          "exception": false,
          "start_time": "2025-04-06T19:48:45.648746",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install peft transformers bitsandbytes accelerate trl datasets -U -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a3a9f5-1f41-4806-ba42-8f77129d28ee",
      "metadata": {
        "id": "63a3a9f5-1f41-4806-ba42-8f77129d28ee"
      },
      "outputs": [],
      "source": [
        "#!pip install scikit-learn sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5388445b",
      "metadata": {
        "id": "5388445b",
        "papermill": {
          "duration": 23.348649,
          "end_time": "2025-04-06T19:49:27.270100",
          "exception": false,
          "start_time": "2025-04-06T19:49:03.921451",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d699be88",
      "metadata": {
        "id": "d699be88",
        "papermill": {
          "duration": 0.007891,
          "end_time": "2025-04-06T19:49:27.286520",
          "exception": false,
          "start_time": "2025-04-06T19:49:27.278629",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Let's first test how the Mistral 7b, pre-trained in Russian (the original model was trained only in English), will cope with this task. Let the model play the role of *an* assistant for an online smartphone store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4559df50",
      "metadata": {
        "id": "4559df50",
        "papermill": {
          "duration": 0.014076,
          "end_time": "2025-04-06T19:49:27.308593",
          "exception": false,
          "start_time": "2025-04-06T19:49:27.294517",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
        "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
        "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
        "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".\n",
        "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
        "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
        "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
        "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
        "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
        "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
        "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
        "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
        "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
        "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
        "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eeb4177",
      "metadata": {
        "id": "9eeb4177",
        "papermill": {
          "duration": 0.015091,
          "end_time": "2025-04-06T19:49:27.331738",
          "exception": false,
          "start_time": "2025-04-06T19:49:27.316647",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Conversation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        message_template=MESSAGE_TEMPLATE,\n",
        "        system_prompt=SYSTEM_PROMPT,\n",
        "        response_template=RESPONSE_TEMPLATE\n",
        "    ):\n",
        "        # Инициализация класса Conversation с шаблонами сообщений и системным запросом.\n",
        "        # message_template - шаблон для каждого сообщения (например, форматирование текста).\n",
        "        # system_prompt - начальный запрос, задающий контекст разговора.\n",
        "        # response_template - шаблон для ответа, который будет добавлен в конце текста.\n",
        "\n",
        "        self.message_template = message_template  # Сохранение шаблона сообщений.\n",
        "        self.response_template = response_template  # Сохранение шаблона ответа.\n",
        "        # Инициализация списка сообщений с первым сообщением от системы, определяющим контекст беседы.\n",
        "        self.messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        }]\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        # Метод для добавления сообщения пользователя в список сообщений.\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def add_bot_message(self, message):\n",
        "        # Метод для добавления сообщения бота в список сообщений.\n",
        "        self.messages.append({\n",
        "            \"role\": \"bot\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def get_prompt(self, tokenizer):\n",
        "        # Метод для формирования финального текста запроса, который будет отправлен для генерации ответа.\n",
        "        final_text = \"\"  # Инициализация переменной для хранения окончательного текста.\n",
        "        for message in self.messages:\n",
        "            # Форматирование каждого сообщения в соответствии с message_template.\n",
        "            message_text = self.message_template.format(**message)\n",
        "            final_text += message_text  # Добавление отформатированного текста к итоговому.\n",
        "\n",
        "        final_text += RESPONSE_TEMPLATE  # Добавление шаблона ответа в конце текста.\n",
        "        return final_text.strip()  # Возвращение итогового текста без лишних пробелов в начале и в конце.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e32befe",
      "metadata": {
        "id": "2e32befe",
        "papermill": {
          "duration": 0.013522,
          "end_time": "2025-04-06T19:49:27.353182",
          "exception": false,
          "start_time": "2025-04-06T19:49:27.339660",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, generation_config):\n",
        "    # Функция для генерации текста на основе заданного промпта с использованием модели и токенизатора.\n",
        "\n",
        "    # Токенизация промпта: превращаем текстовый промпт в числовые представления (токены),\n",
        "    # которые понимает модель. Параметр return_tensors=\"pt\" означает, что данные\n",
        "    # будут возвращены в формате PyTorch тензоров. add_special_tokens=False указывает,\n",
        "    # что специальные токены (например, [CLS], [SEP]) не будут добавлены.\n",
        "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "\n",
        "    # Переносим данные на устройство, на котором работает модель (например, GPU или CPU).\n",
        "    data = {k: v.to(model.device) for k, v in data.items()}\n",
        "\n",
        "    # Генерация выходных идентификаторов (токенов) с использованием модели.\n",
        "    # Параметр generation_config задает конфигурацию генерации, например,\n",
        "    # максимальную длину ответа, температуру и т.д.\n",
        "    output_ids = model.generate(\n",
        "        **data,\n",
        "        generation_config=generation_config\n",
        "    )[0]\n",
        "\n",
        "    # Обрезаем выходные идентификаторы, чтобы исключить исходные токены промпта.\n",
        "    # Таким образом, оставляем только сгенерированный моделью ответ.\n",
        "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
        "\n",
        "    # Декодируем токены обратно в текст, исключая специальные токены, такие как [PAD] или [SEP].\n",
        "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Возвращаем итоговый сгенерированный текст, убирая лишние пробелы в начале и в конце.\n",
        "    return output.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab508c55",
      "metadata": {
        "id": "ab508c55",
        "papermill": {
          "duration": 2.931572,
          "end_time": "2025-04-06T19:49:30.292636",
          "exception": false,
          "start_time": "2025-04-06T19:49:27.361064",
          "status": "completed"
        },
        "tags": [],
        "outputId": "dc807e87-5c16-4f0a-9cf5-0b518faacfe4",
        "colab": {
          "referenced_widgets": [
            "5be33dfe0e4b4ec69c56c62eca4f838b",
            "58cf152e84c547ce996a88b4925ea9ff",
            "8831e63f97734d4c8c212300b01a22b4",
            "8258d40452464c0a8547c605b3f16c41",
            "fe7e5de650ae46cb998f2772ef589db2"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5be33dfe0e4b4ec69c56c62eca4f838b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58cf152e84c547ce996a88b4925ea9ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8831e63f97734d4c8c212300b01a22b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8258d40452464c0a8547c605b3f16c41",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe7e5de650ae46cb998f2772ef589db2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "GenerationConfig {\n",
              "  \"bos_token_id\": 1,\n",
              "  \"do_sample\": true,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"max_new_tokens\": 1536,\n",
              "  \"no_repeat_ngram_size\": 15,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"repetition_penalty\": 1.1,\n",
              "  \"temperature\": 0.2,\n",
              "  \"top_k\": 40,\n",
              "  \"top_p\": 0.9\n",
              "}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
        "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
        "generation_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163a34f3",
      "metadata": {
        "id": "163a34f3",
        "papermill": {
          "duration": 72.625154,
          "end_time": "2025-04-06T19:50:42.927003",
          "exception": false,
          "start_time": "2025-04-06T19:49:30.301849",
          "status": "completed"
        },
        "tags": [],
        "outputId": "8fd9f2c3-21f2-4857-bf0a-7761d929f124",
        "colab": {
          "referenced_widgets": [
            "7296531209764dd1836735136ad0cdb2",
            "39f8bad2ba694b759ff0f096da331e30",
            "bae7d0af2d544b77af18b34d0393fc5b"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7296531209764dd1836735136ad0cdb2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39f8bad2ba694b759ff0f096da331e30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae7d0af2d544b77af18b34d0393fc5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/54.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32002, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): MistralRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Загружаем конфигурацию модели PeftConfig из предобученной модели, используя ее имя.\n",
        "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Загружаем предобученную модель для задачи автозавершения текста (Causal Language Modeling).\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,  # Используем базовую модель из конфигурации.\n",
        "    load_in_8bit=False,  # Загружаем модель с точностью float16, а не 8-битную версию.\n",
        "    torch_dtype=torch.float16,  # Определяем тип данных тензоров как 16-битные числа с плавающей запятой (fp16).\n",
        "    device_map=\"cuda:0\",  # Автоматически распределяем модель по доступным устройствам (например, на GPU, если он доступен).\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, MODEL_NAME, torch_dtype=torch.float16)\n",
        "\n",
        "# Переводим модель в режим оценки, чтобы отключить функции, используемые только при обучении,\n",
        "# такие как dropout, и настроить модель на генерацию предсказаний.\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "899c61ef",
      "metadata": {
        "id": "899c61ef",
        "papermill": {
          "duration": 0.016817,
          "end_time": "2025-04-06T19:50:42.954277",
          "exception": false,
          "start_time": "2025-04-06T19:50:42.937460",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "conversation = Conversation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec6d4a41-e435-45c5-ab3f-adb8d753787a",
      "metadata": {
        "id": "ec6d4a41-e435-45c5-ab3f-adb8d753787a",
        "papermill": {
          "duration": 11.731796,
          "end_time": "2025-04-06T19:50:54.697453",
          "exception": false,
          "start_time": "2025-04-06T19:50:42.965657",
          "status": "completed"
        },
        "tags": [],
        "outputId": "0010cbd7-e608-403e-ada6-6f8228f99c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Чат-бот: К сожалению, я не могу предоставить полный список таблиц с 4K видеокачеством, так как это может быть очень большое количество моделей. Однако, я могу дать вам несколько примеров таблиц с 4K видеокачеством:\n",
            "\n",
            "1. iPad Pro - это таблица Apple, которая имеет 4K видеокачество.\n",
            "2. Samsung Galaxy Tab S7+ - это таблица Samsung, которая также имеет 4K видеокачество.\n",
            "3. Microsoft Surface Pro X - это таблица Microsoft, которая также имеет 4K видеокачество, но только в версии с LTE.\n",
            "\n",
            "Эти таблицы имеют 4K видеокачество благодаря своим высококачественным дисплеям и процессорам. Однако, стоит учесть, что 4K видеокачество требует большего объема памяти и более мощного процессора, чтобы обеспечить плавное воспроизведение видео. Также стоит учитывать, что 4K видеокачество может потреблять больше энергии, что может снизить время работы таблицы.\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    #user_uttr = input(\"Клиент: \")\n",
        "    user_uttr = 'tablets that have 4K video quality'\n",
        "    conversation.add_user_message(user_uttr)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "    output = generate(model, tokenizer, prompt, generation_config)\n",
        "    output = output.split(\"bot\")[0].strip()\n",
        "    print(f\"Чат-бот: {output}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e869787-f93d-4bc3-95a8-2acdbee9b64d",
      "metadata": {
        "id": "3e869787-f93d-4bc3-95a8-2acdbee9b64d"
      },
      "source": [
        "### Важно: код выше я выполнял отдельно с обучением, перезапуская среду, чтобы экономить память"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c097cde8",
      "metadata": {
        "id": "c097cde8",
        "papermill": {
          "duration": 0.0097,
          "end_time": "2025-04-06T19:50:54.717267",
          "exception": false,
          "start_time": "2025-04-06T19:50:54.707567",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Let's start a conversation. Try asking to tell about tablets that have 4K video quality. We see that the bot starts to invent non-existent information, that is, it starts to hallucinate."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b381a0",
      "metadata": {
        "id": "a5b381a0",
        "papermill": {
          "duration": 0.009494,
          "end_time": "2025-04-06T19:50:54.736418",
          "exception": false,
          "start_time": "2025-04-06T19:50:54.726924",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Downloading data for additional fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d70e6c",
      "metadata": {
        "id": "72d70e6c",
        "papermill": {
          "duration": 6.150179,
          "end_time": "2025-04-06T19:51:00.896260",
          "exception": false,
          "start_time": "2025-04-06T19:50:54.746081",
          "status": "completed"
        },
        "tags": [],
        "outputId": "20b30e00-ae9b-413f-e282-9d641f1ade57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-10 08:11:41--  https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT\n",
            "142.250.147.194, 2a00:1450:4025:c01::c2com)... \n",
            "connected. to docs.google.com (docs.google.com)|142.250.147.194|:443... \n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download [following]\n",
            "--2025-04-10 08:11:42--  https://drive.usercontent.google.com/download?id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT&export=download\n",
            "142.250.74.161, 2a00:1450:400f:805::2001drive.usercontent.google.com)... \n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.74.161|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70497894 (67M) [application/octet-stream]\n",
            "Saving to: ‘company_cases.json’\n",
            "\n",
            "company_cases.json  100%[===================>]  67.23M  54.9MB/s    in 1.2s    \n",
            "\n",
            "2025-04-10 08:11:49 (54.9 MB/s) - ‘company_cases.json’ saved [70497894/70497894]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1_xKwkNFisKzDQ1CPPDqvKWdPRzZvO4MT' -O company_cases.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db979d02",
      "metadata": {
        "id": "db979d02",
        "papermill": {
          "duration": 3.721935,
          "end_time": "2025-04-06T19:51:04.628950",
          "exception": false,
          "start_time": "2025-04-06T19:51:00.907015",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b702eb",
      "metadata": {
        "id": "54b702eb",
        "papermill": {
          "duration": 0.971953,
          "end_time": "2025-04-06T19:51:05.611772",
          "exception": false,
          "start_time": "2025-04-06T19:51:04.639819",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "with open(\"company_cases.json\", 'r') as inp:\n",
        "    raw_dataset = json.load(inp)\n",
        "\n",
        "train_raw_dataset, test_raw_dataset = train_test_split(raw_dataset)\n",
        "train_test_raw_dataset = {\"train\": train_raw_dataset, \"test\": test_raw_dataset}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea90f53",
      "metadata": {
        "id": "fea90f53",
        "papermill": {
          "duration": 0.010134,
          "end_time": "2025-04-06T19:51:05.632613",
          "exception": false,
          "start_time": "2025-04-06T19:51:05.622479",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "\n",
        "```\n",
        "\n",
        "Let's convert the dataset into the format used in the Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a54430c",
      "metadata": {
        "id": "5a54430c",
        "papermill": {
          "duration": 0.962037,
          "end_time": "2025-04-06T19:51:06.605480",
          "exception": false,
          "start_time": "2025-04-06T19:51:05.643443",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "raw_dataset_dict = {}\n",
        "for data_type in [\"train\", \"test\"]:\n",
        "    raw_dataset_dict[data_type] = {\n",
        "        \"instruction\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
        "        \"personality\": [element['personality'] for element in train_test_raw_dataset[data_type]],\n",
        "        \"context\": [element['context'] for element in train_test_raw_dataset[data_type]],\n",
        "        \"dialog_start_line\": [element['instruction'] for element in train_test_raw_dataset[data_type]],\n",
        "        \"dialog\": [element['dialog'] for element in train_test_raw_dataset[data_type]]\n",
        "    }\n",
        "\n",
        "train_dataset = Dataset.from_dict(raw_dataset_dict[\"train\"])\n",
        "test_dataset = Dataset.from_dict(raw_dataset_dict[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3bcb88c",
      "metadata": {
        "id": "e3bcb88c",
        "papermill": {
          "duration": 0.00996,
          "end_time": "2025-04-06T19:51:06.625808",
          "exception": false,
          "start_time": "2025-04-06T19:51:06.615848",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Function for formatting the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348a95ea",
      "metadata": {
        "id": "348a95ea",
        "papermill": {
          "duration": 0.016494,
          "end_time": "2025-04-06T19:51:06.652262",
          "exception": false,
          "start_time": "2025-04-06T19:51:06.635768",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def formatting_prompt_func(example, max_seq_length=256):\n",
        "    prompt = f\"<s>system\\n{example['instruction']}\"\n",
        "\n",
        "    if example['personality']:\n",
        "        prompt += f\"\\n{example['personality']}\"\n",
        "\n",
        "    prompt += f\"\\n{example['context']}\"\n",
        "\n",
        "    if example['dialog_start_line']:\n",
        "        prompt += f\"\\n{example['dialog_start_line']}\"\n",
        "\n",
        "    prompt += \"</s>\"\n",
        "\n",
        "    # Add dialog content\n",
        "    for element in example['dialog']:\n",
        "        prompt += f\"<s>{element['role']}\\n{element['content']}</s>\"\n",
        "\n",
        "    # Truncate the prompt if it exceeds max_seq_length\n",
        "    if len(prompt) > max_seq_length:\n",
        "        prompt = prompt[:max_seq_length]\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d31f273",
      "metadata": {
        "id": "7d31f273",
        "papermill": {
          "duration": 0.009957,
          "end_time": "2025-04-06T19:51:06.672819",
          "exception": false,
          "start_time": "2025-04-06T19:51:06.662862",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "For additional training, we will take not the ready Russian-language Mistral, but the pre-trained Mistral on the Open-Orca dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02bc026a",
      "metadata": {
        "id": "02bc026a",
        "papermill": {
          "duration": 1.742087,
          "end_time": "2025-04-06T19:51:08.424857",
          "exception": false,
          "start_time": "2025-04-06T19:51:06.682770",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/Mistral-7B-OpenOrca\")\n",
        "tokenizer.pad_token_id = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5ea7ba4",
      "metadata": {
        "id": "b5ea7ba4",
        "papermill": {
          "duration": 0.010488,
          "end_time": "2025-04-06T19:51:08.446552",
          "exception": false,
          "start_time": "2025-04-06T19:51:08.436064",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Let's make the gradient during training flow only through the tokens of the last replica of the chatbot, and let's make the labels for the remaining tokens equal to -100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af9ecb07",
      "metadata": {
        "id": "af9ecb07",
        "papermill": {
          "duration": 0.017588,
          "end_time": "2025-04-06T19:51:08.475149",
          "exception": false,
          "start_time": "2025-04-06T19:51:08.457561",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "response_template = \"bot\\n\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee778b5",
      "metadata": {
        "papermill": {
          "duration": 0.017267,
          "end_time": "2025-04-06T19:51:08.502769",
          "exception": false,
          "start_time": "2025-04-06T19:51:08.485502",
          "status": "completed"
        },
        "tags": [],
        "id": "2ee778b5"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83504e2e",
      "metadata": {
        "id": "83504e2e",
        "papermill": {
          "duration": 28.375451,
          "end_time": "2025-04-06T19:51:36.888844",
          "exception": false,
          "start_time": "2025-04-06T19:51:08.513393",
          "status": "completed"
        },
        "tags": [],
        "outputId": "7b0a39f0-030b-44ec-eb63-7d60ef8defd8",
        "colab": {
          "referenced_widgets": [
            "2b76f867ce18420cafc5abdf627db4a3"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b76f867ce18420cafc5abdf627db4a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Будем обучать модель в int4 для уменьшения требуемой видеопамяти\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"Open-Orca/Mistral-7B-OpenOrca\",\n",
        "        torch_dtype=torch.float16,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"cuda:0\"\n",
        "    )\n",
        "\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04721dd2",
      "metadata": {
        "id": "04721dd2",
        "papermill": {
          "duration": 0.010708,
          "end_time": "2025-04-06T19:51:36.910723",
          "exception": false,
          "start_time": "2025-04-06T19:51:36.900015",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Initializing LoraConfig."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c94d8fd",
      "metadata": {
        "id": "5c94d8fd",
        "papermill": {
          "duration": 0.022859,
          "end_time": "2025-04-06T19:51:36.944198",
          "exception": false,
          "start_time": "2025-04-06T19:51:36.921339",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=4,  # Number of attention heads per layer in the low-rank approximation.\n",
        "    lora_alpha=32,  # Scaling factor for the low-rank matrix.\n",
        "    lora_dropout=0.05,  # Dropout rate for regularization.\n",
        "    bias=\"none\",  # No bias for LoRA layers.\n",
        "    task_type=\"CAUSAL_LM\"  # The task type we are training for is Causal Language Modeling.\n",
        ")\n",
        "device_name = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_name)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a41bc938",
      "metadata": {
        "id": "a41bc938",
        "papermill": {
          "duration": 0.009986,
          "end_time": "2025-04-06T19:51:36.964446",
          "exception": false,
          "start_time": "2025-04-06T19:51:36.954460",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Setting parameters for additional training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "018218e8",
      "metadata": {
        "id": "018218e8",
        "papermill": {
          "duration": 0.047632,
          "end_time": "2025-04-06T19:51:37.022443",
          "exception": false,
          "start_time": "2025-04-06T19:51:36.974811",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Directory to save the fine-tuned model.\n",
        "    num_train_epochs=0.1,  # Number of training epochs.\n",
        "    per_device_train_batch_size=8,  # Batch size for training.\n",
        "    per_device_eval_batch_size=8,  # Batch size for evaluation.\n",
        "    warmup_steps=500,  # Number of steps for the warmup phase.\n",
        "    weight_decay=0.01,  # Regularization parameter to avoid overfitting.\n",
        "    logging_dir=\"./logs\",  # Directory to store logs.\n",
        "    logging_steps=10,  # Log every 10 steps.\n",
        "    save_steps=50,  # Save the model every 1000 steps.\n",
        "    eval_strategy=\"steps\",  # Evaluate the model after every `save_steps`.\n",
        "    report_to='none'\n",
        "#    max_seq_length=256 Сделано в функции formatting_prompt_func\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca4f5256",
      "metadata": {
        "id": "ca4f5256",
        "papermill": {
          "duration": 0.010226,
          "end_time": "2025-04-06T19:51:37.043383",
          "exception": false,
          "start_time": "2025-04-06T19:51:37.033157",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Disable tokenizer parallelism to avoid deadlocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013df5a9",
      "metadata": {
        "id": "013df5a9",
        "papermill": {
          "duration": 0.01482,
          "end_time": "2025-04-06T19:51:37.068453",
          "exception": false,
          "start_time": "2025-04-06T19:51:37.053633",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602449de-4901-46ac-a21f-69c0854f0d87",
      "metadata": {
        "id": "602449de-4901-46ac-a21f-69c0854f0d87"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61cad3f",
      "metadata": {
        "id": "d61cad3f",
        "papermill": {
          "duration": 0.010416,
          "end_time": "2025-04-06T19:51:37.126262",
          "exception": false,
          "start_time": "2025-04-06T19:51:37.115846",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Starting fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "497b4b0c",
      "metadata": {
        "id": "497b4b0c",
        "papermill": {
          "duration": 44.637345,
          "end_time": "2025-04-06T19:52:21.773984",
          "exception": true,
          "start_time": "2025-04-06T19:51:37.136639",
          "status": "failed"
        },
        "tags": [],
        "outputId": "10bf5e88-fb2f-4152-d273-15bb600acdaf",
        "colab": {
          "referenced_widgets": [
            "ae62028bbf7f483aa123545d2d57114a",
            "3c55ed33d9a4419298baf397b48a5a8b",
            "f21870aaf4854babae53ec459c69f771",
            "fe332a8bc73e43b9bd7beb25248e3734",
            "647a2785d9194712b74ea90927f046f7",
            "be72d7a165604950a093c45cf364b727",
            "a79f94827fb44e419904ae03221135ed",
            "99d422e8c3e3424cbfc8f862397e4c40",
            "bc874307c3b54e3291f7488c25bc49a0",
            "8de71e7355484da59502ff81a23182e3"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae62028bbf7f483aa123545d2d57114a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/8354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c55ed33d9a4419298baf397b48a5a8b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/8354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f21870aaf4854babae53ec459c69f771",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset:   0%|          | 0/8354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe332a8bc73e43b9bd7beb25248e3734",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/8354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "647a2785d9194712b74ea90927f046f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/8354 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be72d7a165604950a093c45cf364b727",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying formatting function to eval dataset:   0%|          | 0/2785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a79f94827fb44e419904ae03221135ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting eval dataset to ChatML:   0%|          | 0/2785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99d422e8c3e3424cbfc8f862397e4c40",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to eval dataset:   0%|          | 0/2785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc874307c3b54e3291f7488c25bc49a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/2785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8de71e7355484da59502ff81a23182e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/2785 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [105/105 26:12, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=105, training_loss=0.0, metrics={'train_runtime': 1573.2026, 'train_samples_per_second': 0.531, 'train_steps_per_second': 0.067, 'total_flos': 4018547205537792.0, 'train_loss': 0.0})"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    formatting_func=formatting_prompt_func,\n",
        "    data_collator=collator,\n",
        "    peft_config=peft_config,\n",
        "    args=training_arguments\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe03e10",
      "metadata": {
        "id": "8fe03e10",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Test the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388af63f",
      "metadata": {
        "id": "388af63f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30319446",
      "metadata": {
        "id": "30319446",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Loading the adapter checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec3b6da",
      "metadata": {
        "id": "5ec3b6da",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
        "ADAPTER_MODEL = \"results/checkpoint-100\"\n",
        "MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
        "RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
        "SYSTEM_PROMPT = \"\"\"Ты – чат-бот технической поддержки Xiaomi Store, который помогает клиенту выбрать наиболее подходящий для него смартфон. Опираясь на описание смартфонов, помоги клиенту выбрать наиболее подходящий для него смартфон. Если ответа на вопрос клиента нет в приведенном описании, ответь \"У меня не достаточно информации для ответа на ваш вопрос. Обратитесь пожалуйста к менеджеру в telegram\".{personality}\n",
        "Описание смартфонов из интернет-магазина сотовой связи Xiaomi Store:\n",
        "1. Смартфон Xiaomi Redmi Note 10 Pro: Этот смартфон оснащен дисплеем Super AMOLED с разрешением 1080 x 2400 пикселей, что обеспечивает четкую и яркую картинку. Он также имеет камеру на 64 Мп с возможностью записи видео в 4K. Процессор Snapdragon 732G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
        "2. Смартфон Xiaomi Mi 11 Lite: Этот смартфон имеет ультратонкий и легкий дизайн, который удобно держать в руке. Он оснащен 6.55-дюймовым AMOLED-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп способна делать качественные фотографии, а быстрый процессор Qualcomm Snapdragon 732G позволяет работать с приложениями плавно.\n",
        "3. Смартфон Xiaomi Redmi 9T: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей, который обеспечивает реалистичную цветопередачу. Камера на 48 Мп делает четкие фото, а процессор Snapdragon 662 обеспечивает быструю работу. Аккумулятор на 6000 мАч позволяет использовать устройство долгое время без подзарядки.\n",
        "4. Смартфон Xiaomi Mi 10T Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 108 Мп, что позволяет делать потрясающие фотографии. Процессор Qualcomm Snapdragon 865 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\n",
        "5. Смартфон Xiaomi Redmi Note 9 Pro: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Камера на 64 Мп с поддержкой искусственного интеллекта позволяет делать яркие и четкие фотографии. Процессор Snapdragon 720G обеспечивает отличную производительность, а аккумулятор на 5020 мАч обеспечивает долгое время работы.\n",
        "6. Смартфон Xiaomi Mi 10T Lite: Этот смартфон оснащен 6.67-дюймовым IPS-дисплеем с разрешением 1080 x 2400 пикселей. Он имеет камеру на 64 Мп, что позволяет делать яркие и детальные фотографии. Процессор Qualcomm Snapdragon 750G ускоряет работу с приложениями, а аккумулятор на 4820 мАч обеспечивает долгую автономность.\n",
        "7. Смартфон Xiaomi Redmi Note 8 Pro: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 64 Мп делает четкие и яркие фото, а процессор MediaTek Helio G90T обеспечивает плавную работу. Аккумулятор на 4500 мАч достаточно емкий для долгого использования.\n",
        "8. Смартфон Xiaomi Mi 10 Pro: Этот смартфон оснащен 6.67-дюймовым AMOLED-дисплеем с разрешением 1080 x 2340 пикселей. Камера на 108 Мп и оптическая стабилизация изображения позволяют делать высококачественные фотографии. Процессор Snapdragon 865 ускоряет работу с приложениями, а аккумулятор на 4500 мАч достаточно емкий.\n",
        "9. Смартфон Xiaomi Redmi 9C: Этот смартфон оснащен 6.53-дюймовым IPS-дисплеем с разрешением 720 x 1600 пикселей. Он имеет камеру на 13 Мп, которая делает четкие фото в хороших условиях освещения. Процессор MediaTek Helio G35 обеспечивает достаточную производительность для базовых задач, а аккумулятор на 5000 мАч обеспечивает долгое время работы.\n",
        "10. Смартфон Xiaomi Mi 11 Ultra: Этот смартфон оснащен 6.81-дюймовым AMOLED-дисплеем с разрешением 1440 x 3200 пикселей. Он имеет камеру на 50 Мп, а также вспомогательные камеры для различных эффектов съемки. Процессор Snapdragon 888 обеспечивает высокую производительность, а аккумулятор на 5000 мАч гарантирует долгое время работы.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92b1b1f",
      "metadata": {
        "id": "f92b1b1f",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class Conversation:\n",
        "    def __init__(\n",
        "        self,\n",
        "        message_template=MESSAGE_TEMPLATE,\n",
        "        system_prompt=SYSTEM_PROMPT,\n",
        "        response_template=RESPONSE_TEMPLATE,\n",
        "        personality=None\n",
        "    ):\n",
        "        self.message_template = message_template\n",
        "        self.response_template = response_template\n",
        "        if personality:\n",
        "            system_prompt = system_prompt.format(personality=personality)\n",
        "        else:\n",
        "            system_prompt = system_prompt.format(personality=\"\")\n",
        "        self.messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": system_prompt\n",
        "        }]\n",
        "\n",
        "    def add_user_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def add_bot_message(self, message):\n",
        "        self.messages.append({\n",
        "            \"role\": \"bot\",\n",
        "            \"content\": message\n",
        "        })\n",
        "\n",
        "    def get_prompt(self, tokenizer):\n",
        "        final_text = \"\"\n",
        "        for message in self.messages:\n",
        "            message_text = self.message_template.format(**message)\n",
        "            final_text += message_text\n",
        "        final_text += RESPONSE_TEMPLATE\n",
        "        return final_text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe4f594",
      "metadata": {
        "id": "9fe4f594",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, generation_config):\n",
        "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    data = {k: v.to(model.device) for k, v in data.items()}\n",
        "    output_ids = model.generate(\n",
        "        **data,\n",
        "        generation_config=generation_config\n",
        "    )[0]\n",
        "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
        "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "    return output.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafc48ab",
      "metadata": {
        "id": "cafc48ab",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n",
        "tokenizer.pad_token_id = 0\n",
        "generation_config = GenerationConfig(\n",
        "    pad_token_id=0,  # The padding token ID.\n",
        "    bos_token_id=1,  # The beginning-of-sequence token ID.\n",
        "    eos_token_id=2,  # The end-of-sequence token ID.\n",
        "    temperature=0.2,  # Temperature to control the randomness of predictions.\n",
        "    top_p=0.2,  # Cumulative probability cutoff for top-p sampling.\n",
        "    max_length=2048  # Maximum length for the model's responses.\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422bc1d8",
      "metadata": {
        "id": "422bc1d8",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "80727ecf-6b83-4fa2-f169-b57f42fcf1ae",
        "colab": {
          "referenced_widgets": [
            "2e37ad6d90304ebd85074b8564365a05"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e37ad6d90304ebd85074b8564365a05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32002, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=4, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=4, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): MistralRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32002, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    load_in_8bit=False,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda:0\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_MODEL, torch_dtype=torch.float16)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cd3241b",
      "metadata": {
        "id": "5cd3241b",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "conversation = Conversation()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "036bc4e3",
      "metadata": {
        "id": "036bc4e3",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "We start the dialogue again and ask about the tablets. Were you able to overcome the hallucinations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f051a5",
      "metadata": {
        "id": "28f051a5",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "outputId": "99e22b03-ad79-435f-b766-390847b66811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Клиент: tablets that have 4K video quality\n",
            "Чат-бот: Пожалуйста, обратитесь к менеджеру в telegram, у меня не достаточно информации для ответа на ваш вопрос.\n",
            "Пожалуйста, обратитесь к менеджеру в telegram, у меня не достаточно информации для ответа на ваш вопрос.\n",
            "\n",
            "\n",
            "Пожалуйста, обратитесь к менеджеру в telegram, у меня не достаточно информации для ответа на ваш вопрос.\n",
            "\n",
            "Пожалуйста, обратитесь к менеджеру в telegram, у меня не достаточно информации для ответа на ваш вопрос.\n",
            "\n",
            "Пожалуйста, обратитесь к менеджеру в telegram, у меня не достаточно информации для ответа на ваш вопрос.\n",
            "\n",
            "Пожалуйста, обрати\n"
          ]
        }
      ],
      "source": [
        "while True:\n",
        "    #user_uttr = input(\"Клиент: \")\n",
        "    user_uttr = 'tablets that have 4K video quality'\n",
        "    print(f\"Клиент: {user_uttr}\")\n",
        "    conversation.add_user_message(user_uttr)\n",
        "    prompt = conversation.get_prompt(tokenizer)\n",
        "    output = generate(model, tokenizer, prompt, generation_config)\n",
        "    output = output.split(\"bot\")[0].replace(\"<s>\", \"\").replace(\"</s>\", \"\").strip()\n",
        "    print(f\"Чат-бот: {output}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ecbed5d",
      "metadata": {
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": [],
        "id": "3ecbed5d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 221.94586,
      "end_time": "2025-04-06T19:52:24.805817",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-04-06T19:48:42.859957",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}