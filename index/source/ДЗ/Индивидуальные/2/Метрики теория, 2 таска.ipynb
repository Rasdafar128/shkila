{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Rules\n",
        "\n",
        "1. All mathematical expressions should be written in **LaTeX** for better clarity and formatting.\n",
        "2. Ensure that the entire notebook can execute seamlessly from start to finish without encountering errors.\n",
        "3. Focus on optimizing the runtime of the code wherever possible to enhance performance.\n",
        "\n",
        "## Notation\n",
        "\n",
        "- $c$: The optimal constant model.  \n",
        "- $y_i$: The target values in the dataset.  \n",
        "- $w_i$: The weights associated with the loss function.\n",
        "- $q$: Quantile value in range $[0, 1]$.\n",
        "\n",
        "# Важно! О формате сдачи\n",
        "\n",
        "* **При решении ноутбука используйте данный шаблон. Не нужно удалять текстовые ячейки c разметкой частей ноутбука и формулировками заданий. Добавлять свои ячейки, при необходимости, конечно можно**\n",
        "* **Везде, где в формулровке задания есть какой-либо вопрос (или просьба вывода), необходимо прописать ответ в ячейку (код или markdown).**\n",
        "* **Наличие кода решения (или аналитического решения - в зависимости от задачи) обязательно. Письменные ответы на вопросы без сопутствующего кода/аналитического решения оцениваются в 0 баллов.**"
      ],
      "metadata": {
        "id": "GOb1Kh1lj609"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1"
      ],
      "metadata": {
        "id": "Fr4xVEQsjeF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description"
      ],
      "metadata": {
        "id": "jfx6HJOj5FLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the loss function:\n",
        "\n",
        "\\begin{align}\n",
        "L = \\sum w_i \\cdot \\left( \\log(y_i) - \\log(c) \\right)^2\n",
        "\\end{align}\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\sum w_i = 1$\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1. **Analytically find the best constant $c$** for the given loss function.\n",
        "2. **Determine the name of the aggregation of $y_i$'s** at the end if $w_1 = w_2 = \\dots = w_n$."
      ],
      "metadata": {
        "id": "QQFbyGOo7Y9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "QujLaGbD5A8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution\n",
        "\n",
        "Let us consider the loss function:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "L = \\sum_{i=1}^n w_i \\cdot \\left( \\log(y_i) - \\log(c) \\right)^2\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^n w_i = 1\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Task 1: Analytically find the optimal \\( c \\)\n",
        "\n",
        "To find \\( c \\) that minimizes the loss function \\( L \\), we take the derivative with respect to \\( c \\) and set it to zero.\n",
        "\n",
        "Expand the square in the loss function:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "L = \\sum_{i=1}^n w_i \\cdot \\left[ \\log^2(y_i) - 2 \\log(y_i) \\log(c) + \\log^2(c) \\right].\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Separate the terms in the summation:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "L = \\sum_{i=1}^n w_i \\cdot \\log^2(y_i) - 2 \\log(c) \\sum_{i=1}^n w_i \\cdot \\log(y_i) + \\sum_{i=1}^n w_i \\cdot \\log^2(c).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Since the sum of the weights is equal to 1, the last term simplifies, and \\( L \\) becomes:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "L = \\sum_{i=1}^n w_i \\cdot \\log^2(y_i) - 2 \\log(c) \\sum_{i=1}^n w_i \\log(y_i) + \\log^2(c).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Now, calculate the derivative of \\( L \\) with respect to \\( c \\):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial c} = -2 \\cdot \\frac{\\sum_{i=1}^n w_i \\log(y_i)}{c} + \\frac{2 \\cdot \\log(c)}{c}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Set the derivative to zero:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{-2 \\sum_{i=1}^n w_i \\log(y_i)}{c} + \\frac{2 \\log(c)}{c} = 0.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Multiply through by \\( c \\) and simplify:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "-2 \\sum_{i=1}^n w_i \\log(y_i) + 2 \\log(c) = 0.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\log(c) = \\sum_{i=1}^n w_i \\log(y_i).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Exponentiate both sides to solve for \\( c \\):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\exp\\left( \\sum_{i=1}^n w_i \\log(y_i) \\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## Task 2: Determine the aggregation type of \\( y_i \\) when \\( w_1 = w_2 = ... = w_n \\)\n",
        "\n",
        "The problem specifies that:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^n w_i = 1,\n",
        "$$\n",
        "\n",
        "meaning the weights \\( w_i \\) sum to one. If all weights are equal, their value is distributed evenly among all elements. Since there are \\( n \\) weights, each weight equals:\n",
        "\n",
        "$$\n",
        "w_i = \\frac{1}{n}.\n",
        "$$\n",
        "\n",
        "Substitute this value into the formula for \\( c \\):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\exp\\left( \\sum_{i=1}^n \\frac{1}{n} \\log(y_i) \\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\exp\\left( \\frac{1}{n} \\sum_{i=1}^n \\log(y_i) \\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The term inside the exponent is the arithmetic mean of the logarithms of \\( y_i \\). Using logarithmic properties, this can be rewritten as:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\sqrt[n]{y_1 \\cdot y_2 \\cdot \\dots \\cdot y_n}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "This value is known as the **geometric mean** of the numbers \\( y_1, y_2, ..., y_n \\).\n",
        "\n",
        "---\n",
        "\n",
        "### Final Answer\n",
        "\n",
        "1. The optimal constant \\( c \\) is:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\exp\\left( \\sum_{i=1}^n w_i \\log(y_i) \\right).\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "2. If \\( w_1 = w_2 = ... = w_n \\), then \\( c \\) becomes:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "c = \\sqrt[n]{y_1 \\cdot y_2 \\cdot \\dots \\cdot y_n}.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Thus, \\( c \\) represents the **geometric mean** of \\( y_i \\) when all weights are equal."
      ],
      "metadata": {
        "id": "1hVENP-85OaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2"
      ],
      "metadata": {
        "id": "iXSZRuzZ7Mnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description"
      ],
      "metadata": {
        "id": "A14Tq1s470G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the **quantile loss function** $L$, and prove that the optimal constant $c$ corresponds to the quantile $q$ of the data $y_1, \\dots, y_n$.\n",
        "\n",
        "The quantile loss function is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "L =\n",
        "\\begin{cases}\n",
        "q \\cdot (y_i - c), & \\text{if } y_i \\geq c \\\\\n",
        "(1-q) \\cdot (c - y_i), & \\text{if } y_i < c\n",
        "\\end{cases}\n",
        "\\end{align}\n",
        "\n",
        "**Hint**:\n",
        "\n",
        "Proof for optimal MAE constant on [this page](https://ds100.org/course-notes/constant_model_loss_transformations/loss_transformations.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "vTM5F2rZ78Zm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution"
      ],
      "metadata": {
        "id": "HohX1KQk70Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{1. Definition of the Quantile Loss Function:}\n",
        "$$\n",
        "The quantile loss function is defined as:\n",
        "\n",
        "$$\n",
        "L(c) = \\sum_{i=1}^{n} L_i(c),\n",
        "$$\n",
        "where\n",
        "\n",
        "$$\n",
        "L_i(c) =\n",
        "\\begin{cases}\n",
        "q \\cdot (y_i - c), & \\text{if } y_i \\geq c, \\\\\n",
        "(1 - q) \\cdot (c - y_i), & \\text{if } y_i < c.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{2. Minimization of the Loss Function:}\n",
        "$$\n",
        "To find the optimal constant \\( c \\), we need to minimize the total loss function \\( L(c) \\). Since \\( L_i(c) \\) is a piecewise linear function with respect to \\( c \\), we compute the derivative of the loss function and find its minimum by setting the derivative to zero.\n",
        "\n",
        "Let’s examine how the loss function \\( L(c) \\) behaves depending on whether \\( c \\) is less than or greater than some data \\( y_i \\).\n",
        "\n",
        "$$\n",
        "\\text{3. Differentiation by Parts:}\n",
        "$$\n",
        "For \\( y_i >= c \\), the loss function is linear with respect to \\( c \\):\n",
        "\n",
        "$$\n",
        "L_i(c) = q \\cdot (y_i - c),\n",
        "$$\n",
        "and the derivative is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_i(c)}{\\partial c} = -q.\n",
        "$$\n",
        "\n",
        "For \\( y_i < c \\), the loss function is also linear with respect to \\( c \\):\n",
        "\n",
        "$$\n",
        "L_i(c) = (1 - q) \\cdot (c - y_i),\n",
        "$$\n",
        "and the derivative is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L_i(c)}{\\partial c} = 1 - q.\n",
        "$$\n",
        "\n",
        "Thus, the derivative of the total loss function with respect to \\( c \\) is:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(c)}{\\partial c} = \\sum_{i=1}^{n}\n",
        "\\begin{cases}\n",
        "-q, & \\text{if } y_i \\geq c, \\\\\n",
        "1 - q, & \\text{if } y_i < c.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{4. Solution for Optimal \\( c \\):}\n",
        "$$\n",
        "To minimize \\( L(c) \\), we set the derivative equal to zero:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(c)}{\\partial c} = 0.\n",
        "$$\n",
        "\n",
        "This implies that the number of terms with derivative \\( -q \\) must balance the number of terms with derivative \\( 1 - q \\). Let \\( k \\) be the number of data points \\( y_i \\) such that \\( y_i \\geq c \\). We then get the equation:\n",
        "\n",
        "$$\n",
        "k \\cdot (-q) + (n - k) \\cdot (1 - q) = 0.\n",
        "$$\n",
        "\n",
        "Simplifying this equation:\n",
        "\n",
        "$$\n",
        "-kq + (n - k)(1 - q) = 0,\n",
        "$$\n",
        "$$\n",
        "-kq + n(1 - q) - k(1 - q) = 0,\n",
        "$$\n",
        "$$\n",
        "k \\cdot [q + (1 - q)] = n(1 - q),\n",
        "$$\n",
        "$$\n",
        "k = \\lfloor nq \\rfloor.\n",
        "$$\n",
        "\n",
        "Thus, the optimal constant \\( c \\) corresponds to the value \\( y_k \\), which is the \\( q \\)-th quantile of the data \\( y_1, \\dots, y_n \\).\n",
        "\n",
        "---\n",
        "\n",
        "### Final Answer\n",
        "\n",
        "$$\n",
        "\\text{The optimal constant } c \\text{ that minimizes the quantile loss function is the quantile } q \\text{ of the data,}\n",
        "$$\n",
        "$$\n",
        "\\text{and it corresponds to the } \\lfloor nq \\rfloor \\text{-th order statistic in the dataset } y_1, \\dots, y_n.\n",
        "$$\n",
        "$$\n",
        "\\text{This confirms the proof.}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "hoMeumwu7RGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Problem 3"
      ],
      "metadata": {
        "id": "VKg7Sh-H_XlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def bruteforce_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    \"\"\"\n",
        "    Finds the optimal constant c by brute force using a specified loss function.\n",
        "\n",
        "    Parameters:\n",
        "    y (np.ndarray): Array of target values.\n",
        "    loss_func (callable): A function that computes the loss given y and c.\n",
        "    tol (float): The step size for generating potential c values between min(y) and max(y).\n",
        "\n",
        "    Returns:\n",
        "    float: The optimal constant c that minimizes the loss.\n",
        "    \"\"\"\n",
        "    # Generate potential c values between min(y) and max(y) with a given tolerance\n",
        "    c_values = np.arange(np.min(y), np.max(y), tol)\n",
        "\n",
        "    # Compute loss for each potential c\n",
        "    loss_values = [loss_func(y, c) for c in c_values]\n",
        "\n",
        "    # Find the c with the minimum loss\n",
        "    best_c = c_values[np.argmin(loss_values)]\n",
        "\n",
        "\n",
        "    return best_c\n",
        "\n",
        "# Define the loss function\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "# Example usage of the function\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "optimal_c = bruteforce_constant(y, loss, tol=0.001)\n",
        "min_loss = loss(y, optimal_c)\n",
        "print(f\"The optimal constant c is: {optimal_c}\")\n",
        "print(f\"The minimum loss is: {min_loss}\")"
      ],
      "metadata": {
        "id": "gsCBQe8C-U-i",
        "outputId": "a2368c04-774c-47de-d32c-5326dd8a971d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal constant c is: 4.1969999999996475\n",
            "The minimum loss is: 243.96167948452023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4"
      ],
      "metadata": {
        "id": "H1UPKOpBLb5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize import minimize\n",
        "import numpy as np\n",
        "\n",
        "def minimize_constant(y: np.ndarray, loss_func: callable, tol: float) -> float:\n",
        "    \"\"\"\n",
        "    Finds the optimal constant c using scipy's minimize function with a specified loss function.\n",
        "\n",
        "    Parameters:\n",
        "    y (np.ndarray): Array of target values.\n",
        "    loss_func (callable): A function that computes the loss given y and c.\n",
        "    tol (float): The tolerance for the optimization process.\n",
        "\n",
        "    Returns:\n",
        "    float: The optimal constant c that minimizes the loss.\n",
        "    \"\"\"\n",
        "    # Initial guess for the constant c, we can start with the mean of y\n",
        "    initial_guess = np.mean(y)\n",
        "\n",
        "    # Minimize the loss function using scipy's minimize\n",
        "    result = minimize(lambda c: loss_func(y, c), initial_guess, tol=tol)\n",
        "\n",
        "    # Return the optimal c found by minimize\n",
        "    return result.x[0]\n",
        "\n",
        "# Define the loss function\n",
        "def loss(y, c):\n",
        "    return np.sum(np.log(np.cosh(y - c)))\n",
        "\n",
        "# Example usage of the function\n",
        "y = np.array([1, 2, 3, 4, 55, 99, 100])\n",
        "optimal_c = minimize_constant(y, loss, tol=0.001)\n",
        "min_loss = loss(y, optimal_c)\n",
        "print(f\"The optimal constant c is: {optimal_c}\")\n",
        "print(f\"The minimum loss is: {min_loss}\")"
      ],
      "metadata": {
        "id": "NI-R1YHaIRx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "787c9917-ecbb-4ab2-838a-43ab04918e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal constant c is: 4.197181683003827\n",
            "The minimum loss is: 243.96167941355066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 5\n",
        "\n",
        "## Description\n",
        "In a multiclass classification problem, we often compare a model’s performance with a simple baseline. Two baseline approaches are:\n",
        "\n",
        "1. **Always predict the most frequent class** (also known as the “constant baseline”).  \n",
        "2. **Randomly sample a class** for every object with probabilities proportional to the class frequencies in the training set.\n",
        "\n",
        "The goal of this task is to determine which of these two approaches yields a higher **Accuracy** on a given dataset.\n",
        "\n",
        "You are provided with:  \n",
        "1. A set of objects \\\\( X \\\\) and their true class labels \\\\( y \\\\), where \\\\( y \\in \\{C_1, C_2, \\dots, C_k\\} \\\\).  \n",
        "2. The frequencies (or proportions) of each class in the training data.\n",
        "\n",
        "You need to:  \n",
        "1. **Explain** how to implement these two baseline approaches (always predicting the most frequent class vs. sampling a class according to its frequency).  \n",
        "2. **Calculate** the expected Accuracy for each approach.  \n",
        "3. **Compare** the results to determine which method is more likely to produce a higher Accuracy."
      ],
      "metadata": {
        "id": "xBJbvAj27Bld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution\n",
        "\n",
        "### Task Description\n",
        "\n",
        "There are two basic approaches for multiclass classification:\n",
        "\n",
        "#### 1. Constant Base Approach\n",
        "\n",
        "Suppose we have a dataset with \\( k \\) classes, and the frequencies of the classes in the training set are denoted by \\( p_1, p_2, \\dots, p_k \\), where \\( p_i \\) is the probability that a randomly chosen object belongs to class \\( C_i \\). These probabilities must satisfy the normalization condition:\n",
        "\n",
        "$$\n",
        "\\sum_{i=1}^{k} p_i = 1.\n",
        "$$\n",
        "\n",
        "In the constant base approach, the model always selects the class with the highest frequency, i.e., the class \\( C_1 \\), whose probability is \\( p_1 \\).\n",
        "\n",
        "#### Expected Accuracy\n",
        "\n",
        "The model will always predict the most frequent class \\( C_1 \\). Thus, the accuracy of this approach is equal to the probability that the true class is also \\( C_1 \\). In other words:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{constant}} = p_1.\n",
        "$$\n",
        "\n",
        "#### Log-Loss\n",
        "\n",
        "The log-loss is calculated as the average value for all objects in the test set. For the constant base approach, where the same class \\( C_1 \\) is always predicted, the log-loss can be written as:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss}_{\\text{constant}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(p_1),\n",
        "$$\n",
        "\n",
        "which represents the average logarithmic loss over all objects in the test set, where the probability of being in class \\( C_1 \\) is \\( p_1 \\).\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Class Selection Proportional to Frequency Base Approach\n",
        "\n",
        "In this approach, the model selects a class randomly with a probability proportional to the frequency of that class in the training set. Thus, for each object, the probability that it will be classified into class \\( C_i \\) is \\( p_i \\).\n",
        "\n",
        "#### Expected Accuracy\n",
        "\n",
        "The model predicts classes with probabilities proportional to their frequencies in the training set. The probability that the model correctly predicts the class for a given object will depend on how frequently that class appears in the data. For each object, the probability of a correct prediction is \\( p_i^2 \\). Thus, the expected accuracy of this approach is:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{sampling}} = \\sum_{i=1}^{k} p_i^2.\n",
        "$$\n",
        "\n",
        "#### Log-Loss\n",
        "\n",
        "For the base approach with class selection proportional to frequency, the log-loss is calculated using the following formula:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss}_{\\text{sampling}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} p_j \\log(p_j),\n",
        "$$\n",
        "\n",
        "which represents the sum of the logarithms of all probabilities for each class, weighted by \\( p_j \\), the probability of an object belonging to class \\( C_j \\).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. Comparison of Accuracy and Log-Loss\n",
        "\n",
        "##### Accuracy Comparison\n",
        "\n",
        "Now we have two formulas for accuracy:\n",
        "\n",
        "- For the constant approach:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{constant}} = p_1,\n",
        "$$\n",
        "\n",
        "where \\( p_1 \\) is the probability that a randomly chosen object belongs to the most frequent class.\n",
        "\n",
        "- For the sampling approach:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{sampling}} = \\sum_{i=1}^{k} p_i^2.\n",
        "$$\n",
        "\n",
        "To understand which approach is better, we compare these two values. Let's analyze this as follows:\n",
        "\n",
        "### Comparison of Accuracy and Log-Loss\n",
        "\n",
        "#### Accuracy Comparison\n",
        "\n",
        "Now we have two formulas for accuracy:\n",
        "\n",
        "- For the constant approach:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{constant}} = p_1\n",
        "$$\n",
        "\n",
        "where \\( p_1 \\) is the probability that a randomly chosen object belongs to the most frequent class.\n",
        "\n",
        "- For the sampling approach:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy}_{\\text{sampling}} = \\sum_{i=1}^{k} p_i^2\n",
        "$$\n",
        "\n",
        "To understand which approach is better, we compare these two values.\n",
        "\n",
        "##### Case 1: When the constant approach performs better:\n",
        "\n",
        "If:\n",
        "\n",
        "$$\n",
        "p_1^2 \\geq \\sum_{i=1}^{k} p_i^2,\n",
        "$$\n",
        "\n",
        "then the constant approach will perform better. This often happens when one class dominates the rest, i.e., \\( p_1 \\) is significantly larger than all other \\( p_i \\).\n",
        "\n",
        "##### Case 2: When the frequency-based approach performs better:\n",
        "\n",
        "If:\n",
        "\n",
        "$$\n",
        "p_1^2 < \\sum_{i=1}^{k} p_i^2,\n",
        "$$\n",
        "\n",
        "then the frequency-based approach will perform better. This holds true when the classes are more evenly distributed and there is no clearly dominant class. In this case, randomly selecting the class according to the frequency distribution might lead to better accuracy.\n",
        "\n",
        "\n",
        "##### Log-Loss Comparison\n",
        "\n",
        "Now, let's compare the log-loss for the two approaches:\n",
        "\n",
        "- For the constant approach:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss}_{\\text{constant}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\log(p_1).\n",
        "$$\n",
        "\n",
        "- For the sampling approach:\n",
        "\n",
        "$$\n",
        "\\text{Log-Loss}_{\\text{sampling}} = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k} p_j \\log(p_j).\n",
        "$$\n",
        "\n",
        "Thus, for the class selection proportional to frequency approach, the log-loss will typically be lower if the classes are more evenly distributed, since the probability of error decreases when the class selection is more balanced.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. When is Each Approach Better?\n",
        "\n",
        "The constant approach will be better when one class is overwhelmingly dominant (i.e., \\( p_1 \\) is much larger than all other \\( p_i \\)).\n",
        "\n",
        "The frequency-based approach will be better when the classes are roughly equally represented in the training data, i.e., when all \\( p_i \\) are similar.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Final answer\n",
        "- If there is a class that significantly dominates the data (i.e., \\( p_1 \\) is much greater than all other \\( p_i \\)), the constant approach will be more effective both in terms of accuracy and log-loss.\n",
        "- If the classes are more evenly distributed, the frequency-based approach will yield better results both in terms of accuracy and log-loss.\n",
        "\n",
        "This comparison allows for an informed choice between the two approaches based on the specific characteristics of the data and their frequency distribution.\n"
      ],
      "metadata": {
        "id": "Cabuv0Ti79qz"
      }
    }
  ]
}